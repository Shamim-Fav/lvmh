import streamlit as st
import requests
import pandas as pd
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
import numpy as np
import re 
from typing import List, Optional

# --- CRITICAL FIX: Ensure this is the first Streamlit command ---
st.set_page_config(
Â  Â  page_title="ðŸ’¼ LVMH Job Scraper",
    layout="centered"
)
# ---------------------------------------------------------------

# ================== CONFIG ==================
URL = "https://www.lvmh.com/api/search"
REGIONS_ALL = ["America", "Asia Pacific", "Europe", "Middle East / Africa"]
HITS_PER_PAGE = 50
SESSION_MAX_AGE = 30 * 60 # 30 minutes

# ================== GLOBAL SESSION ==================
SESSION = None
SESSION_TIMESTAMP = None

def create_session() -> requests.Session:
Â  Â  """Create or refresh the requests session with automatic cookies."""
Â  Â  global SESSION, SESSION_TIMESTAMP
Â  Â  now = time.time()
Â  Â  if SESSION and (now - SESSION_TIMESTAMP) < SESSION_MAX_AGE:
Â  Â  Â  Â  return SESSIONÂ  # reuse existing session

Â  Â  session = requests.Session()
Â  Â  retry_strategy = Retry(
Â  Â  Â  Â  total=5,
Â  Â  Â  Â  backoff_factor=1,
Â  Â  Â  Â  status_forcelist=[429, 500, 502, 503, 504],
Â  Â  Â  Â  allowed_methods=["GET", "POST"]
Â  Â  )
Â  Â  adapter = HTTPAdapter(max_retries=retry_strategy)
Â  Â  session.mount("https://", adapter)

Â  Â  session.headers.update({
Â  Â  Â  Â  "accept": "*/*",
Â  Â  Â  Â  "content-type": "application/json",
Â  Â  Â  Â  "origin": "https://www.lvmh.com",
Â  Â  Â  Â  "referer": "https://www.lvmh.com/en/join-us/our-job-offers",
Â  Â  Â  Â  "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36"
Â  Â  })

Â  Â  # Refresh cookies by visiting the job offers page
Â  Â  try:
Â  Â  Â  Â  session.get("https://www.lvmh.com/en/join-us/our-job-offers", timeout=15)
Â  Â  except requests.RequestException as e:
Â  Â  Â  Â  st.warning(f"Could not initialize session cookies: {e}")

Â  Â  SESSION = session
Â  Â  SESSION_TIMESTAMP = time.time()
Â  Â  return session

# -------------------------------------------------------------------
## ðŸ” Fetching Functions

def fetch_jobs_page(session: requests.Session, regions: List[str], keyword: Optional[str] = None, page: int = 0) -> dict:
Â  Â  """Fetch a single page of jobs from the API."""
Â  Â  facet_filters = [[f"geographicAreaFilter:{r}" for r in regions]]
Â  Â  payload = {
Â  Â  Â  Â  "queries": [
Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  "indexName": "PRD-en-us-timestamp-desc",
Â  Â  Â  Â  Â  Â  Â  Â  "params": {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "facetFilters": facet_filters,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "facets": ["businessGroupFilter", "cityFilter", "contractFilter", "countryRegionFilter"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "filters": "category:job",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "highlightPostTag": "__/ais-highlight__",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "highlightPreTag": "__ais-highlight__",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "hitsPerPage": HITS_PER_PAGE,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "maxValuesPerFacet": 100,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "page": page,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "query": keyword if keyword else ""
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  ]
Â  Â  }
Â  Â  resp = session.post(URL, json=payload, timeout=30)
Â  Â  resp.raise_for_status()
Â  Â  return resp.json()

def extract_jobs(data: dict) -> List[dict]:
Â  Â  """Extract jobs from the JSON response."""
Â  Â  jobs = []
Â  Â  for query_result in data.get("results", []):
Â  Â  Â  Â  for hit in query_result.get("hits", []):
Â  Â  Â  Â  Â  Â  jobs.append(hit)
Â  Â  return jobs

@st.cache_data(ttl=3600)Â  # Cache the scraped data for 1 hour
def scrape_jobs(keyword: Optional[str], selected_regions: List[str], _progress_bar=None) -> pd.DataFrame:
Â  Â  """Scrape all jobs for given regions and keyword, with optional progress bar."""
Â  Â  session = create_session()
Â  Â  all_jobs = []
Â  Â  regions_to_use = selected_regions if selected_regions else REGIONS_ALL
Â  Â  page = 0
Â  Â  total_fetched = 0
Â  Â  
Â  Â  while True:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  data = fetch_jobs_page(session, regions_to_use, keyword, page)
Â  Â  Â  Â  Â  Â  jobs = extract_jobs(data)
Â  Â  Â  Â  Â  Â  if not jobs:
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â  all_jobs.extend(jobs)
Â  Â  Â  Â  Â  Â  total_fetched += len(jobs)
Â  Â  Â  Â  Â  Â  page += 1
Â  Â  Â  Â  Â  Â  if _progress_bar:
Â  Â  Â  Â  Â  Â  Â  Â  # Max 1.0 for Streamlit progress bar, assuming max ~2500 jobs
Â  Â  Â  Â  Â  Â  Â  Â  _progress_bar.progress(min(total_fetched / 2500, 1.0)) 
Â  Â  Â  Â  Â  Â  time.sleep(0.5)Â  # polite delay
Â  Â  Â  Â  except requests.RequestException as e:
Â  Â  Â  Â  Â  Â  st.error(f"Request failed on page {page}. Stopping scrape: {e}")
Â  Â  Â  Â  Â  Â  break
Â  Â  return pd.DataFrame(all_jobs)

# -------------------------------------------------------------------
## âœ¨ Data Processing Functions

def fix_encoding(text: str) -> str:
Â  Â  """Attempts to fix double-encoded UTF-8 strings."""
Â  Â  if isinstance(text, str):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  return text.encode('latin1').decode('utf8')
Â  Â  Â  Â  except (UnicodeDecodeError, UnicodeEncodeError):
Â  Â  Â  Â  Â  Â  return text
Â  Â  return text

def create_filtered_df(df: pd.DataFrame) -> pd.DataFrame:
Â  Â  """Applies all cleaning, merging, slug creation, column renaming logic, and sets final column order."""
Â  Â  if df.empty:
Â  Â  Â  Â  return pd.DataFrame()

Â  Â  # 1. Encoding Fix
Â  Â  for col in ['city', 'description', 'name', 'profile', 'jobResponsabilities', 'salary']:
Â  Â  Â  Â  if col in df.columns:
Â  Â  Â  Â  Â  Â  df[col] = df[col].apply(fix_encoding)

Â  Â  # 2. Salary Placeholder
Â  Â  df['Salary Range'] = df['salary'] if 'salary' in df.columns else ''

Â  Â  # 3. Merge Full Description
Â  Â  if all(col in df.columns for col in ['profile', 'jobResponsabilities', 'description']):
Â  Â  Â  Â  df['FullDescription'] = ''
Â  Â  Â  Â  for col in ['profile', 'jobResponsabilities', 'description']:
Â  Â  Â  Â  Â  Â  df[col] = df[col].astype(str).str.strip()
Â  Â  Â  Â  Â  Â  df['FullDescription'] += "\n\n--- " + col.upper() + " ---\n" + df[col]
Â  Â  Â  Â  df['description'] = df['FullDescription'].str.strip()

Â  Â  # Prevent formula interpretation in Excel
Â  Â  if 'description' in df.columns:
Â  Â  Â  Â  df['description'] = np.where(df['description'].str.startswith(('=', '+', '-')),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â "'" + df['description'], df['description'])

Â  Â  # 4. Slug Creation (FIXED ORDER: Company-Name-Location)
Â  Â  if all(col in df.columns for col in ['name', 'maison', 'city']):
Â  Â  Â  Â  cleaned_maison = df['maison'].astype(str).str.strip()
Â  Â  Â  Â  cleaned_name = df['name'].astype(str).str.strip()
Â  Â  Â  Â  cleaned_city = df['city'].astype(str).str.strip()
Â  Â  Â  Â  
Â  Â  Â  Â  df['Slug'] = (cleaned_maison.str.lower().str.replace(' ', '-') + '-' +
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cleaned_name.str.lower().str.replace(' ', '-') + '-' +
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cleaned_city.str.lower().str.replace(' ', '-'))
Â  Â  Â  Â  
Â  Â  Â  Â  # Robust cleaning: remove non-alphanumeric chars (except hyphen) and clean up multiple hyphens
Â  Â  Â  Â  df['Slug'] = df['Slug'].str.replace(r'[^a-z0-9\-]+', '-', regex=True)
Â  Â  Â  Â  df['Slug'] = df['Slug'].str.replace(r'[\-]+', '-', regex=True).str.strip('-')
Â  Â  else:
Â  Â  Â  Â  df['Slug'] = ''

Â  Â  # 5. Column Mapping
Â  Â  column_map = {
Â  Â  Â  Â  'name': 'Name',
Â  Â  Â  Â  'maison': 'Company',
Â  Â  Â  Â  'contract': 'Type',
Â  Â  Â  Â  'description': 'Description',
Â  Â  Â  Â  'city': 'Location',
Â  Â  Â  Â  'functionFilter': 'Industry',
Â  Â  Â  Â  'fullTimePartTime': 'Level',
Â  Â  Â  Â  'link': 'Apply URL',
Â  Â  Â  Â  'Slug': 'Slug',
Â  Â  Â  Â  'Salary Range': 'Salary Range'
Â  Â  }
Â  Â  # Select and rename columns that exist in the raw dataframe
Â  Â  existing_cols = [col for col in column_map.keys() if col in df.columns]
Â  Â  df_filtered = df[existing_cols].rename(columns=column_map)

Â  Â  # 6. Add Blank Columns
Â  Â  blank_columns = ['Collection ID', 'Locale ID', 'Item ID', 'Archived', 'Draft',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'Created On', 'Updated On', 'Published On', 'CMS ID', 'Access', 'Salary', 'Deadline']
Â  Â  for col in blank_columns:
Â  Â  Â  Â  df_filtered[col] = ''

Â  Â  # 7. Clean Algolia highlights
Â  Â  if 'Description' in df_filtered.columns:
Â  Â  Â  Â  df_filtered['Description'] = df_filtered['Description'].astype(str).str.replace('__ais-highlight__', '').str.replace('__/ais-highlight__', '')

Â  Â  # 8. Final Column Order
Â  Â  final_order = ['Name', 'Slug', 'Collection ID', 'Locale ID', 'Item ID', 'Archived',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'Draft', 'Created On', 'Updated On', 'Published On', 'CMS ID',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'Company', 'Type', 'Description', 'Salary Range', 'Access',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 'Location', 'Industry', 'Level', 'Salary', 'Deadline', 'Apply URL']
Â  Â  
Â  Â  # Reindex the dataframe to enforce the exact order
Â  Â  df_filtered = df_filtered.reindex(columns=[col for col in final_order if col in df_filtered.columns])

Â  Â  return df_filtered

@st.cache_data
def convert_df_to_csv(df: pd.DataFrame) -> bytes:
Â  Â  """Converts DataFrame to CSV string with BOM for Excel compatibility."""
Â  Â  # Use utf-8-sig which automatically includes the BOM
Â  Â  return df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')

# -------------------------------------------------------------------
## ðŸ’» Streamlit UI

st.title("ðŸ’¼ LVMH Job Scraper")Â 

keyword_input = st.text_input("Job Title / Keywords (leave blank for all)")
regions_input = st.multiselect("Select Regions", REGIONS_ALL, default=REGIONS_ALL)

if st.button("Fetch Jobs"):
Â  Â  progress_bar = st.progress(0)
Â  Â  with st.spinner("Scraping jobs... this may take a few minutes..."):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  df_raw = scrape_jobs(keyword_input.strip() if keyword_input else None, regions_input, _progress_bar=progress_bar)
Â  Â  Â  Â  Â  Â  progress_bar.empty()

Â  Â  Â  Â  Â  Â  if not df_raw.empty:
Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"Found {len(df_raw)} jobs!")

Â  Â  Â  Â  Â  Â  Â  Â  df_filtered = create_filtered_df(df_raw.copy())
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(df_filtered, use_container_width=True)

Â  Â  Â  Â  Â  Â  Â  Â  # --- TWO SEPARATE DOWNLOAD BUTTONS (CSV with BOM) ---
Â  Â  Â  Â  Â  Â  Â  Â  csv_raw = convert_df_to_csv(df_raw)
Â  Â  Â  Â  Â  Â  Â  Â  csv_filtered = convert_df_to_csv(df_filtered)

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Download CSVs")
Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Download Full Raw Data CSV",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=csv_raw,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name="lvmh_jobs_FULL_RAW.csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv"
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Download Filtered CSV",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data=csv_filtered,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name="lvmh_jobs_FILTERED_CLEAN.csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv"
Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("No jobs found. Try different search criteria.")

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"An unexpected error occurred: {e}")
